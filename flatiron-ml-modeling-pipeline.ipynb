{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "intro",
      "metadata": {},
      "source": [
        "# Flatiron Modeling Pipeline — Data Exploration\n",
        "\n",
        "Initial notebook to load the core dataset and perform a quick health check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "imports",
      "metadata": {},
      "outputs": [],
      "source": [
        "import io\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "SECTION_DIVIDER = '=' * 40\n",
        "SUB_SECTION_DIVIDER = '-' * 40\n",
        "\n",
        "def print_heading(title: str):\n",
        "    \"\"\"Print a visually distinct major section header.\"\"\"\n",
        "    print(f\"\\n{SECTION_DIVIDER}\\n{title}\\n{SECTION_DIVIDER}\\n\")\n",
        "\n",
        "def print_sub_heading(title: str):\n",
        "    \"\"\"Print a visually distinct sub-section header.\"\"\"\n",
        "    print(f\"\\n{SUB_SECTION_DIVIDER}\\n{title}\\n{SUB_SECTION_DIVIDER}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "load-data",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "========================================\n",
            "Loading Raw Dataset\n",
            "========================================\n",
            "\n",
            "Reading dataset from: data/Dataset.csv\n",
            "Dataset loaded successfully with 71,250 rows and 18 columns.\n",
            "\n",
            "========================================\n",
            "Original Data Overview\n",
            "========================================\n",
            "\n",
            "\n",
            "----------------------------------------\n",
            "Data Head (First 5 Rows)\n",
            "----------------------------------------\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hash</th>\n",
              "      <th>nonce</th>\n",
              "      <th>transaction_index</th>\n",
              "      <th>from_address</th>\n",
              "      <th>to_address</th>\n",
              "      <th>value</th>\n",
              "      <th>gas</th>\n",
              "      <th>gas_price</th>\n",
              "      <th>input</th>\n",
              "      <th>receipt_cumulative_gas_used</th>\n",
              "      <th>receipt_gas_used</th>\n",
              "      <th>block_timestamp</th>\n",
              "      <th>block_number</th>\n",
              "      <th>block_hash</th>\n",
              "      <th>from_scam</th>\n",
              "      <th>to_scam</th>\n",
              "      <th>from_category</th>\n",
              "      <th>to_category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0x253ec84729f5c11437f5346830e6bdc9857171b16097...</td>\n",
              "      <td>370394</td>\n",
              "      <td>4</td>\n",
              "      <td>0x267be1c1d684f78cb4f6a176c4911b741e4ffdc0</td>\n",
              "      <td>0xd707ac0098599006f0857e8da4c950795645ba01</td>\n",
              "      <td>1.800000e+18</td>\n",
              "      <td>30000</td>\n",
              "      <td>2.205000e+10</td>\n",
              "      <td>0x</td>\n",
              "      <td>129358</td>\n",
              "      <td>21000</td>\n",
              "      <td>2017-10-16 05:26:53 UTC</td>\n",
              "      <td>4370008</td>\n",
              "      <td>0x5e14d30d2155c0cdd65044d7e0f296373f3e92f65ebd...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0xfa5f1d7715562adb1e408fdbbbbfc033ae4fadca1e07...</td>\n",
              "      <td>229082</td>\n",
              "      <td>85</td>\n",
              "      <td>0x304cc179719bc5b05418d6f7f6783abe45d83090</td>\n",
              "      <td>0xb8f6e76d34d4877732dfefa6b6a4e2a834a9092b</td>\n",
              "      <td>7.500000e+15</td>\n",
              "      <td>90000</td>\n",
              "      <td>2.060947e+10</td>\n",
              "      <td>0x</td>\n",
              "      <td>5415387</td>\n",
              "      <td>21000</td>\n",
              "      <td>2017-10-16 05:28:49 UTC</td>\n",
              "      <td>4370014</td>\n",
              "      <td>0x900efe010b41b8b6b135db041422fac80b69b96d93c2...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0x37928f1d12ee769d0876695aaa815c13d705f8b1e376...</td>\n",
              "      <td>229095</td>\n",
              "      <td>31</td>\n",
              "      <td>0x304cc179719bc5b05418d6f7f6783abe45d83090</td>\n",
              "      <td>0x5ec492652d7b92421680f21169ff6a8dbbc622ab</td>\n",
              "      <td>7.500000e+15</td>\n",
              "      <td>90000</td>\n",
              "      <td>2.060947e+10</td>\n",
              "      <td>0x</td>\n",
              "      <td>1369625</td>\n",
              "      <td>21000</td>\n",
              "      <td>2017-10-16 05:29:19 UTC</td>\n",
              "      <td>4370015</td>\n",
              "      <td>0xfe7d1bc1c6257f92bacb0f8b1266c9894dda99b42e78...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0xaad886f331a2fb40495cdf5faa10882852fcd324f064...</td>\n",
              "      <td>229126</td>\n",
              "      <td>78</td>\n",
              "      <td>0x304cc179719bc5b05418d6f7f6783abe45d83090</td>\n",
              "      <td>0xdf01b231b53d8b4cbb1b092a8dc86730cebf4aa9</td>\n",
              "      <td>7.500000e+15</td>\n",
              "      <td>90000</td>\n",
              "      <td>2.060947e+10</td>\n",
              "      <td>0x</td>\n",
              "      <td>2464655</td>\n",
              "      <td>21000</td>\n",
              "      <td>2017-10-16 05:29:43 UTC</td>\n",
              "      <td>4370016</td>\n",
              "      <td>0x1d132e9cd35e9e2cf8350258401b43474772eb1904f1...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0x3dbe36fd1a1b01b5e1b343ff78ec1ec189feee5acf46...</td>\n",
              "      <td>229131</td>\n",
              "      <td>83</td>\n",
              "      <td>0x304cc179719bc5b05418d6f7f6783abe45d83090</td>\n",
              "      <td>0xaa4b9693c54029d180134161cc99e98b76cea249</td>\n",
              "      <td>7.500000e+15</td>\n",
              "      <td>90000</td>\n",
              "      <td>2.060947e+10</td>\n",
              "      <td>0x</td>\n",
              "      <td>2569655</td>\n",
              "      <td>21000</td>\n",
              "      <td>2017-10-16 05:29:43 UTC</td>\n",
              "      <td>4370016</td>\n",
              "      <td>0x1d132e9cd35e9e2cf8350258401b43474772eb1904f1...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                hash   nonce  \\\n",
              "0  0x253ec84729f5c11437f5346830e6bdc9857171b16097...  370394   \n",
              "1  0xfa5f1d7715562adb1e408fdbbbbfc033ae4fadca1e07...  229082   \n",
              "2  0x37928f1d12ee769d0876695aaa815c13d705f8b1e376...  229095   \n",
              "3  0xaad886f331a2fb40495cdf5faa10882852fcd324f064...  229126   \n",
              "4  0x3dbe36fd1a1b01b5e1b343ff78ec1ec189feee5acf46...  229131   \n",
              "\n",
              "   transaction_index                                from_address  \\\n",
              "0                  4  0x267be1c1d684f78cb4f6a176c4911b741e4ffdc0   \n",
              "1                 85  0x304cc179719bc5b05418d6f7f6783abe45d83090   \n",
              "2                 31  0x304cc179719bc5b05418d6f7f6783abe45d83090   \n",
              "3                 78  0x304cc179719bc5b05418d6f7f6783abe45d83090   \n",
              "4                 83  0x304cc179719bc5b05418d6f7f6783abe45d83090   \n",
              "\n",
              "                                   to_address         value    gas  \\\n",
              "0  0xd707ac0098599006f0857e8da4c950795645ba01  1.800000e+18  30000   \n",
              "1  0xb8f6e76d34d4877732dfefa6b6a4e2a834a9092b  7.500000e+15  90000   \n",
              "2  0x5ec492652d7b92421680f21169ff6a8dbbc622ab  7.500000e+15  90000   \n",
              "3  0xdf01b231b53d8b4cbb1b092a8dc86730cebf4aa9  7.500000e+15  90000   \n",
              "4  0xaa4b9693c54029d180134161cc99e98b76cea249  7.500000e+15  90000   \n",
              "\n",
              "      gas_price input  receipt_cumulative_gas_used  receipt_gas_used  \\\n",
              "0  2.205000e+10    0x                       129358             21000   \n",
              "1  2.060947e+10    0x                      5415387             21000   \n",
              "2  2.060947e+10    0x                      1369625             21000   \n",
              "3  2.060947e+10    0x                      2464655             21000   \n",
              "4  2.060947e+10    0x                      2569655             21000   \n",
              "\n",
              "           block_timestamp  block_number  \\\n",
              "0  2017-10-16 05:26:53 UTC       4370008   \n",
              "1  2017-10-16 05:28:49 UTC       4370014   \n",
              "2  2017-10-16 05:29:19 UTC       4370015   \n",
              "3  2017-10-16 05:29:43 UTC       4370016   \n",
              "4  2017-10-16 05:29:43 UTC       4370016   \n",
              "\n",
              "                                          block_hash  from_scam  to_scam  \\\n",
              "0  0x5e14d30d2155c0cdd65044d7e0f296373f3e92f65ebd...          0        0   \n",
              "1  0x900efe010b41b8b6b135db041422fac80b69b96d93c2...          0        0   \n",
              "2  0xfe7d1bc1c6257f92bacb0f8b1266c9894dda99b42e78...          0        0   \n",
              "3  0x1d132e9cd35e9e2cf8350258401b43474772eb1904f1...          0        0   \n",
              "4  0x1d132e9cd35e9e2cf8350258401b43474772eb1904f1...          0        0   \n",
              "\n",
              "  from_category to_category  \n",
              "0           NaN         NaN  \n",
              "1           NaN         NaN  \n",
              "2           NaN         NaN  \n",
              "3           NaN         NaN  \n",
              "4           NaN         NaN  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------\n",
            "Dataset Info (Schema & Non-Null Counts)\n",
            "----------------------------------------\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 71250 entries, 0 to 71249\n",
            "Data columns (total 18 columns):\n",
            " #   Column                       Non-Null Count  Dtype  \n",
            "---  ------                       --------------  -----  \n",
            " 0   hash                         71250 non-null  object \n",
            " 1   nonce                        71250 non-null  int64  \n",
            " 2   transaction_index            71250 non-null  int64  \n",
            " 3   from_address                 71250 non-null  object \n",
            " 4   to_address                   71250 non-null  object \n",
            " 5   value                        71250 non-null  float64\n",
            " 6   gas                          71250 non-null  int64  \n",
            " 7   gas_price                    71250 non-null  float64\n",
            " 8   input                        71250 non-null  object \n",
            " 9   receipt_cumulative_gas_used  71250 non-null  int64  \n",
            " 10  receipt_gas_used             71250 non-null  int64  \n",
            " 11  block_timestamp              71250 non-null  object \n",
            " 12  block_number                 71250 non-null  int64  \n",
            " 13  block_hash                   71250 non-null  object \n",
            " 14  from_scam                    71250 non-null  int64  \n",
            " 15  to_scam                      71250 non-null  int64  \n",
            " 16  from_category                2628 non-null   object \n",
            " 17  to_category                  11649 non-null  object \n",
            "dtypes: float64(2), int64(8), object(8)\n",
            "memory usage: 9.8+ MB\n",
            "\n",
            "\n",
            "----------------------------------------\n",
            "Null Value Counts (Absolute)\n",
            "----------------------------------------\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "hash                               0\n",
              "nonce                              0\n",
              "transaction_index                  0\n",
              "from_address                       0\n",
              "to_address                         0\n",
              "value                              0\n",
              "gas                                0\n",
              "gas_price                          0\n",
              "input                              0\n",
              "receipt_cumulative_gas_used        0\n",
              "receipt_gas_used                   0\n",
              "block_timestamp                    0\n",
              "block_number                       0\n",
              "block_hash                         0\n",
              "from_scam                          0\n",
              "to_scam                            0\n",
              "from_category                  68622\n",
              "to_category                    59601\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------\n",
            "Descriptive Statistics (Numeric Columns)\n",
            "----------------------------------------\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>nonce</th>\n",
              "      <th>transaction_index</th>\n",
              "      <th>value</th>\n",
              "      <th>gas</th>\n",
              "      <th>gas_price</th>\n",
              "      <th>receipt_cumulative_gas_used</th>\n",
              "      <th>receipt_gas_used</th>\n",
              "      <th>block_number</th>\n",
              "      <th>from_scam</th>\n",
              "      <th>to_scam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>7.125000e+04</td>\n",
              "      <td>71250.000000</td>\n",
              "      <td>7.125000e+04</td>\n",
              "      <td>7.125000e+04</td>\n",
              "      <td>7.125000e+04</td>\n",
              "      <td>7.125000e+04</td>\n",
              "      <td>7.125000e+04</td>\n",
              "      <td>7.125000e+04</td>\n",
              "      <td>71250.000000</td>\n",
              "      <td>71250.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3.078713e+05</td>\n",
              "      <td>72.694035</td>\n",
              "      <td>1.002473e+19</td>\n",
              "      <td>6.834562e+04</td>\n",
              "      <td>3.449287e+10</td>\n",
              "      <td>2.946858e+06</td>\n",
              "      <td>2.627084e+04</td>\n",
              "      <td>5.209526e+06</td>\n",
              "      <td>0.036884</td>\n",
              "      <td>0.163495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>9.978455e+05</td>\n",
              "      <td>64.650247</td>\n",
              "      <td>2.858156e+20</td>\n",
              "      <td>1.335602e+05</td>\n",
              "      <td>1.250177e+11</td>\n",
              "      <td>2.444175e+06</td>\n",
              "      <td>5.131794e+04</td>\n",
              "      <td>7.509279e+05</td>\n",
              "      <td>0.188479</td>\n",
              "      <td>0.369819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>2.100000e+04</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>2.100000e+04</td>\n",
              "      <td>2.100000e+04</td>\n",
              "      <td>4.370008e+06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>3.000000e+00</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>2.490000e+16</td>\n",
              "      <td>2.100000e+04</td>\n",
              "      <td>8.000000e+09</td>\n",
              "      <td>7.744545e+05</td>\n",
              "      <td>2.100000e+04</td>\n",
              "      <td>4.900821e+06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>8.500000e+01</td>\n",
              "      <td>56.000000</td>\n",
              "      <td>2.480000e+17</td>\n",
              "      <td>3.500000e+04</td>\n",
              "      <td>2.000000e+10</td>\n",
              "      <td>2.338859e+06</td>\n",
              "      <td>2.100000e+04</td>\n",
              "      <td>5.067095e+06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>7.515425e+04</td>\n",
              "      <td>109.000000</td>\n",
              "      <td>1.500000e+18</td>\n",
              "      <td>9.000000e+04</td>\n",
              "      <td>5.060947e+10</td>\n",
              "      <td>4.820330e+06</td>\n",
              "      <td>2.100000e+04</td>\n",
              "      <td>5.285231e+06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.372920e+07</td>\n",
              "      <td>379.000000</td>\n",
              "      <td>6.000000e+22</td>\n",
              "      <td>9.344755e+06</td>\n",
              "      <td>3.000000e+13</td>\n",
              "      <td>9.991824e+06</td>\n",
              "      <td>6.787079e+06</td>\n",
              "      <td>9.185954e+06</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              nonce  transaction_index         value           gas  \\\n",
              "count  7.125000e+04       71250.000000  7.125000e+04  7.125000e+04   \n",
              "mean   3.078713e+05          72.694035  1.002473e+19  6.834562e+04   \n",
              "std    9.978455e+05          64.650247  2.858156e+20  1.335602e+05   \n",
              "min    0.000000e+00           0.000000  0.000000e+00  2.100000e+04   \n",
              "25%    3.000000e+00          21.000000  2.490000e+16  2.100000e+04   \n",
              "50%    8.500000e+01          56.000000  2.480000e+17  3.500000e+04   \n",
              "75%    7.515425e+04         109.000000  1.500000e+18  9.000000e+04   \n",
              "max    1.372920e+07         379.000000  6.000000e+22  9.344755e+06   \n",
              "\n",
              "          gas_price  receipt_cumulative_gas_used  receipt_gas_used  \\\n",
              "count  7.125000e+04                 7.125000e+04      7.125000e+04   \n",
              "mean   3.449287e+10                 2.946858e+06      2.627084e+04   \n",
              "std    1.250177e+11                 2.444175e+06      5.131794e+04   \n",
              "min    1.000000e+00                 2.100000e+04      2.100000e+04   \n",
              "25%    8.000000e+09                 7.744545e+05      2.100000e+04   \n",
              "50%    2.000000e+10                 2.338859e+06      2.100000e+04   \n",
              "75%    5.060947e+10                 4.820330e+06      2.100000e+04   \n",
              "max    3.000000e+13                 9.991824e+06      6.787079e+06   \n",
              "\n",
              "       block_number     from_scam       to_scam  \n",
              "count  7.125000e+04  71250.000000  71250.000000  \n",
              "mean   5.209526e+06      0.036884      0.163495  \n",
              "std    7.509279e+05      0.188479      0.369819  \n",
              "min    4.370008e+06      0.000000      0.000000  \n",
              "25%    4.900821e+06      0.000000      0.000000  \n",
              "50%    5.067095e+06      0.000000      0.000000  \n",
              "75%    5.285231e+06      0.000000      0.000000  \n",
              "max    9.185954e+06      1.000000      1.000000  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "========================================\n",
            "Category Distribution Overview\n",
            "========================================\n",
            "\n",
            "\n",
            "----------------------------------------\n",
            "from_category Value Counts\n",
            "----------------------------------------\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "from_category\n",
              "NaN         68622\n",
              "Scamming     1402\n",
              "Phishing     1225\n",
              "Fake ICO        1\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------\n",
            "to_category Value Counts\n",
            "----------------------------------------\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "to_category\n",
              "NaN         59601\n",
              "Scamming     9758\n",
              "Phishing     1891\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "DATA_PATH = 'data/Dataset.csv'\n",
        "\n",
        "print_heading('Loading Raw Dataset')\n",
        "print(f'Reading dataset from: {DATA_PATH}')\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "print(f'Dataset loaded successfully with {len(df):,} rows and {len(df.columns)} columns.')\n",
        "\n",
        "print_heading('Original Data Overview')\n",
        "\n",
        "print_sub_heading('Data Head (First 5 Rows)')\n",
        "pd.set_option('display.max_columns', None)\n",
        "display(df.head())\n",
        "pd.reset_option('display.max_columns')\n",
        "\n",
        "print_sub_heading('Dataset Info (Schema & Non-Null Counts)')\n",
        "buffer = io.StringIO()\n",
        "df.info(buf=buffer)\n",
        "print(buffer.getvalue())\n",
        "\n",
        "print_sub_heading('Null Value Counts (Absolute)')\n",
        "display(df.isnull().sum())\n",
        "\n",
        "print_sub_heading('Descriptive Statistics (Numeric Columns)')\n",
        "display(df.describe())\n",
        "\n",
        "print_heading('Category Distribution Overview')\n",
        "print_sub_heading('from_category Value Counts')\n",
        "display(df['from_category'].value_counts(dropna=False))\n",
        "\n",
        "print_sub_heading('to_category Value Counts')\n",
        "display(df['to_category'].value_counts(dropna=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91b9c154",
      "metadata": {},
      "source": [
        "### Original Ethereum Transaction Data — Feature Dictionary\n",
        "\n",
        "| Field | Type | Meaning | Use | Notes |\n",
        "|---|---|---|---|---|\n",
        "| hash | string | Unique transaction hash | context | Not modeled directly; can be used as row ID |\n",
        "| nonce | int | Per-sender transaction count at time of tx | dropped | Not used in current feature set |\n",
        "| transaction_index | int | Position of tx within block | dropped | Block-local ordering only |\n",
        "| from_address | string | Sender address | analysis | Used as key to build per-address features |\n",
        "| to_address | string | Recipient address | analysis | Used as key to build per-address features |\n",
        "| value | float | Transferred ETH amount in wei | analysis | Aggregated into incoming/outgoing amount features |\n",
        "| gas | int | Gas limit specified for tx | analysis | Used for avg gas limit per address |\n",
        "| gas_price | float | Gas price offered (wei per gas unit) | analysis | Used for avg gas price per address |\n",
        "| input | string | Hex calldata / payload | dropped | Not parsed in this project |\n",
        "| receipt_cumulative_gas_used | int | Total gas used in block up to this tx | dropped | Not used in current features |\n",
        "| receipt_gas_used | int | Gas used by this tx alone | dropped | Redundant with other gas behavior for this analysis |\n",
        "| block_timestamp | string → datetime | Block time for tx | analysis | Parsed to UTC; basis for all time/sequence features |\n",
        "| block_number | int | Block height containing tx | dropped | Highly collinear with timestamp; not modeled directly |\n",
        "| block_hash | string | Hash of containing block | dropped | Not used in current features |\n",
        "| from_scam | int (0/1) | Source is labeled scam address | analysis | Used to construct per-address Scam label |\n",
        "| to_scam | int (0/1) | Destination is labeled scam address | analysis | Used to construct per-address Scam label |\n",
        "| from_category | string | Labeled category for sender | analysis | Used to flag scam/fraud/phish categories |\n",
        "| to_category | string | Labeled category for recipient | analysis | Used to flag scam/fraud/phish categories |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "41a22446",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "========================================\n",
            "Preparing Transaction Data for Feature Engineering\n",
            "========================================\n",
            "\n",
            "Initial row count: 71250\n",
            "True unparseable timestamps: 14250\n",
            "Examples: ['2017-10-16 08:31:44+00:00' '2017-10-16 11:08:20+00:00'\n",
            " '2017-10-16 16:40:24+00:00' '2017-10-16 19:47:29+00:00'\n",
            " '2017-10-16 22:04:16+00:00' '2017-10-17 05:53:01+00:00'\n",
            " '2017-10-17 08:59:11+00:00' '2017-10-17 09:00:58+00:00'\n",
            " '2017-10-17 15:59:43+00:00' '2017-10-17 16:00:01+00:00']\n",
            "\n",
            "========================================\n",
            "Numeric Column Diagnostics\n",
            "========================================\n",
            "\n",
            "value: invalid=0\n",
            "gas: invalid=0\n",
            "gas_price: invalid=0\n",
            "receipt_cumulative_gas_used: invalid=0\n",
            "receipt_gas_used: invalid=0\n",
            "Numeric cleaning complete. Rows retained: 71250\n",
            "\n",
            "========================================\n",
            "Degree & Amount Features\n",
            "========================================\n",
            "\n",
            "\n",
            "========================================\n",
            "Time-Based Features\n",
            "========================================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/nf/tn5571297x1_h_3dc_3ggtww0000gp/T/ipykernel_2362/359786373.py:266: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  time_stats = long_df.groupby('Address').apply(compute_time_stats)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "========================================\n",
            "Advanced Temporal Behavior Features\n",
            "========================================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/nf/tn5571297x1_h_3dc_3ggtww0000gp/T/ipykernel_2362/359786373.py:296: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  burst = long_df.groupby('Address').apply(compute_burstiness)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "========================================\n",
            "Gas-Based Features\n",
            "========================================\n",
            "\n",
            "\n",
            "========================================\n",
            "Graph Features (Clustering Coefficient)\n",
            "========================================\n",
            "\n",
            "\n",
            "========================================\n",
            "Scam Labeling\n",
            "========================================\n",
            "\n",
            "\n",
            "----------------------------------------\n",
            "Final Feature Table Sample\n",
            "----------------------------------------\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>all_degree</th>\n",
              "      <th>in_degree</th>\n",
              "      <th>out_degree</th>\n",
              "      <th>unique in_degree</th>\n",
              "      <th>unique out_degree</th>\n",
              "      <th>Avg amount incoming</th>\n",
              "      <th>Total amount incoming</th>\n",
              "      <th>Max amount incoming</th>\n",
              "      <th>Min amount incoming</th>\n",
              "      <th>Avg amount outgoing</th>\n",
              "      <th>...</th>\n",
              "      <th>Outgoing count</th>\n",
              "      <th>In/Out Ratio</th>\n",
              "      <th>Hour mean</th>\n",
              "      <th>Hour entropy</th>\n",
              "      <th>Last seen</th>\n",
              "      <th>Recency</th>\n",
              "      <th>Avg gas price</th>\n",
              "      <th>Avg gas limit</th>\n",
              "      <th>Clustering coefficient</th>\n",
              "      <th>Scam</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Address</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0x267be1c1d684f78cb4f6a176c4911b741e4ffdc0</th>\n",
              "      <td>1702</td>\n",
              "      <td>1</td>\n",
              "      <td>1701</td>\n",
              "      <td>1</td>\n",
              "      <td>1567</td>\n",
              "      <td>1.000000e+16</td>\n",
              "      <td>1.000000e+16</td>\n",
              "      <td>1.000000e+16</td>\n",
              "      <td>1.000000e+16</td>\n",
              "      <td>1.106280e+19</td>\n",
              "      <td>...</td>\n",
              "      <td>1701.0</td>\n",
              "      <td>0.000588</td>\n",
              "      <td>13.166863</td>\n",
              "      <td>4.422034</td>\n",
              "      <td>14214201.0</td>\n",
              "      <td>48060563.0</td>\n",
              "      <td>6.459977e+10</td>\n",
              "      <td>31551.116334</td>\n",
              "      <td>8.150218e-07</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0x304cc179719bc5b05418d6f7f6783abe45d83090</th>\n",
              "      <td>315</td>\n",
              "      <td>189</td>\n",
              "      <td>126</td>\n",
              "      <td>189</td>\n",
              "      <td>116</td>\n",
              "      <td>2.987973e+17</td>\n",
              "      <td>5.647270e+19</td>\n",
              "      <td>3.190000e+19</td>\n",
              "      <td>4.079820e+12</td>\n",
              "      <td>2.405875e+17</td>\n",
              "      <td>...</td>\n",
              "      <td>126.0</td>\n",
              "      <td>1.488189</td>\n",
              "      <td>8.555556</td>\n",
              "      <td>3.740761</td>\n",
              "      <td>11165717.0</td>\n",
              "      <td>51109047.0</td>\n",
              "      <td>7.040660e+09</td>\n",
              "      <td>48600.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0x323b10c39732f689a4763fbeebc2347a24b863e2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>4.960000e+16</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>-0.000000</td>\n",
              "      <td>283.0</td>\n",
              "      <td>62274481.0</td>\n",
              "      <td>2.060947e+10</td>\n",
              "      <td>21000.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0x44de2e2b1f378d51e0d2a2ae5ebe4d427ab955a9</th>\n",
              "      <td>466</td>\n",
              "      <td>80</td>\n",
              "      <td>386</td>\n",
              "      <td>80</td>\n",
              "      <td>343</td>\n",
              "      <td>2.340084e+18</td>\n",
              "      <td>1.872067e+20</td>\n",
              "      <td>1.020000e+20</td>\n",
              "      <td>3.470000e+16</td>\n",
              "      <td>2.282073e+17</td>\n",
              "      <td>...</td>\n",
              "      <td>386.0</td>\n",
              "      <td>0.206718</td>\n",
              "      <td>11.257511</td>\n",
              "      <td>4.502845</td>\n",
              "      <td>1257200.0</td>\n",
              "      <td>61017564.0</td>\n",
              "      <td>6.430423e+09</td>\n",
              "      <td>103832.618026</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0xd063435d7cab1a792e1d56f7aab04313b3d87179</th>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "      <td>23</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>3.647826e+18</td>\n",
              "      <td>...</td>\n",
              "      <td>23.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7.260870</td>\n",
              "      <td>2.381774</td>\n",
              "      <td>730529.0</td>\n",
              "      <td>61544235.0</td>\n",
              "      <td>6.110000e+10</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 34 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            all_degree  in_degree  out_degree  \\\n",
              "Address                                                                         \n",
              "0x267be1c1d684f78cb4f6a176c4911b741e4ffdc0        1702          1        1701   \n",
              "0x304cc179719bc5b05418d6f7f6783abe45d83090         315        189         126   \n",
              "0x323b10c39732f689a4763fbeebc2347a24b863e2           1          0           1   \n",
              "0x44de2e2b1f378d51e0d2a2ae5ebe4d427ab955a9         466         80         386   \n",
              "0xd063435d7cab1a792e1d56f7aab04313b3d87179          23          0          23   \n",
              "\n",
              "                                            unique in_degree  \\\n",
              "Address                                                        \n",
              "0x267be1c1d684f78cb4f6a176c4911b741e4ffdc0                 1   \n",
              "0x304cc179719bc5b05418d6f7f6783abe45d83090               189   \n",
              "0x323b10c39732f689a4763fbeebc2347a24b863e2                 0   \n",
              "0x44de2e2b1f378d51e0d2a2ae5ebe4d427ab955a9                80   \n",
              "0xd063435d7cab1a792e1d56f7aab04313b3d87179                 0   \n",
              "\n",
              "                                            unique out_degree  \\\n",
              "Address                                                         \n",
              "0x267be1c1d684f78cb4f6a176c4911b741e4ffdc0               1567   \n",
              "0x304cc179719bc5b05418d6f7f6783abe45d83090                116   \n",
              "0x323b10c39732f689a4763fbeebc2347a24b863e2                  1   \n",
              "0x44de2e2b1f378d51e0d2a2ae5ebe4d427ab955a9                343   \n",
              "0xd063435d7cab1a792e1d56f7aab04313b3d87179                 23   \n",
              "\n",
              "                                            Avg amount incoming  \\\n",
              "Address                                                           \n",
              "0x267be1c1d684f78cb4f6a176c4911b741e4ffdc0         1.000000e+16   \n",
              "0x304cc179719bc5b05418d6f7f6783abe45d83090         2.987973e+17   \n",
              "0x323b10c39732f689a4763fbeebc2347a24b863e2         0.000000e+00   \n",
              "0x44de2e2b1f378d51e0d2a2ae5ebe4d427ab955a9         2.340084e+18   \n",
              "0xd063435d7cab1a792e1d56f7aab04313b3d87179         0.000000e+00   \n",
              "\n",
              "                                            Total amount incoming  \\\n",
              "Address                                                             \n",
              "0x267be1c1d684f78cb4f6a176c4911b741e4ffdc0           1.000000e+16   \n",
              "0x304cc179719bc5b05418d6f7f6783abe45d83090           5.647270e+19   \n",
              "0x323b10c39732f689a4763fbeebc2347a24b863e2           0.000000e+00   \n",
              "0x44de2e2b1f378d51e0d2a2ae5ebe4d427ab955a9           1.872067e+20   \n",
              "0xd063435d7cab1a792e1d56f7aab04313b3d87179           0.000000e+00   \n",
              "\n",
              "                                            Max amount incoming  \\\n",
              "Address                                                           \n",
              "0x267be1c1d684f78cb4f6a176c4911b741e4ffdc0         1.000000e+16   \n",
              "0x304cc179719bc5b05418d6f7f6783abe45d83090         3.190000e+19   \n",
              "0x323b10c39732f689a4763fbeebc2347a24b863e2         0.000000e+00   \n",
              "0x44de2e2b1f378d51e0d2a2ae5ebe4d427ab955a9         1.020000e+20   \n",
              "0xd063435d7cab1a792e1d56f7aab04313b3d87179         0.000000e+00   \n",
              "\n",
              "                                            Min amount incoming  \\\n",
              "Address                                                           \n",
              "0x267be1c1d684f78cb4f6a176c4911b741e4ffdc0         1.000000e+16   \n",
              "0x304cc179719bc5b05418d6f7f6783abe45d83090         4.079820e+12   \n",
              "0x323b10c39732f689a4763fbeebc2347a24b863e2         0.000000e+00   \n",
              "0x44de2e2b1f378d51e0d2a2ae5ebe4d427ab955a9         3.470000e+16   \n",
              "0xd063435d7cab1a792e1d56f7aab04313b3d87179         0.000000e+00   \n",
              "\n",
              "                                            Avg amount outgoing  ...  \\\n",
              "Address                                                          ...   \n",
              "0x267be1c1d684f78cb4f6a176c4911b741e4ffdc0         1.106280e+19  ...   \n",
              "0x304cc179719bc5b05418d6f7f6783abe45d83090         2.405875e+17  ...   \n",
              "0x323b10c39732f689a4763fbeebc2347a24b863e2         4.960000e+16  ...   \n",
              "0x44de2e2b1f378d51e0d2a2ae5ebe4d427ab955a9         2.282073e+17  ...   \n",
              "0xd063435d7cab1a792e1d56f7aab04313b3d87179         3.647826e+18  ...   \n",
              "\n",
              "                                            Outgoing count  In/Out Ratio  \\\n",
              "Address                                                                    \n",
              "0x267be1c1d684f78cb4f6a176c4911b741e4ffdc0          1701.0      0.000588   \n",
              "0x304cc179719bc5b05418d6f7f6783abe45d83090           126.0      1.488189   \n",
              "0x323b10c39732f689a4763fbeebc2347a24b863e2             1.0      0.000000   \n",
              "0x44de2e2b1f378d51e0d2a2ae5ebe4d427ab955a9           386.0      0.206718   \n",
              "0xd063435d7cab1a792e1d56f7aab04313b3d87179            23.0      0.000000   \n",
              "\n",
              "                                            Hour mean  Hour entropy  \\\n",
              "Address                                                               \n",
              "0x267be1c1d684f78cb4f6a176c4911b741e4ffdc0  13.166863      4.422034   \n",
              "0x304cc179719bc5b05418d6f7f6783abe45d83090   8.555556      3.740761   \n",
              "0x323b10c39732f689a4763fbeebc2347a24b863e2   5.000000     -0.000000   \n",
              "0x44de2e2b1f378d51e0d2a2ae5ebe4d427ab955a9  11.257511      4.502845   \n",
              "0xd063435d7cab1a792e1d56f7aab04313b3d87179   7.260870      2.381774   \n",
              "\n",
              "                                             Last seen     Recency  \\\n",
              "Address                                                              \n",
              "0x267be1c1d684f78cb4f6a176c4911b741e4ffdc0  14214201.0  48060563.0   \n",
              "0x304cc179719bc5b05418d6f7f6783abe45d83090  11165717.0  51109047.0   \n",
              "0x323b10c39732f689a4763fbeebc2347a24b863e2       283.0  62274481.0   \n",
              "0x44de2e2b1f378d51e0d2a2ae5ebe4d427ab955a9   1257200.0  61017564.0   \n",
              "0xd063435d7cab1a792e1d56f7aab04313b3d87179    730529.0  61544235.0   \n",
              "\n",
              "                                            Avg gas price  Avg gas limit  \\\n",
              "Address                                                                    \n",
              "0x267be1c1d684f78cb4f6a176c4911b741e4ffdc0   6.459977e+10   31551.116334   \n",
              "0x304cc179719bc5b05418d6f7f6783abe45d83090   7.040660e+09   48600.000000   \n",
              "0x323b10c39732f689a4763fbeebc2347a24b863e2   2.060947e+10   21000.000000   \n",
              "0x44de2e2b1f378d51e0d2a2ae5ebe4d427ab955a9   6.430423e+09  103832.618026   \n",
              "0xd063435d7cab1a792e1d56f7aab04313b3d87179   6.110000e+10   42000.000000   \n",
              "\n",
              "                                            Clustering coefficient  Scam  \n",
              "Address                                                                   \n",
              "0x267be1c1d684f78cb4f6a176c4911b741e4ffdc0            8.150218e-07     0  \n",
              "0x304cc179719bc5b05418d6f7f6783abe45d83090            0.000000e+00     0  \n",
              "0x323b10c39732f689a4763fbeebc2347a24b863e2            0.000000e+00     0  \n",
              "0x44de2e2b1f378d51e0d2a2ae5ebe4d427ab955a9            0.000000e+00     0  \n",
              "0xd063435d7cab1a792e1d56f7aab04313b3d87179            0.000000e+00     0  \n",
              "\n",
              "[5 rows x 34 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total addresses: 73034\n",
            "Total scam labels: 169\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Feature Engineering (Bulletproof + Row-Safe)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "\n",
        "SECTION_DIVIDER = '=' * 40\n",
        "SUB_SECTION_DIVIDER = '-' * 40\n",
        "\n",
        "def print_heading(title: str):\n",
        "    print(f\"\\n{SECTION_DIVIDER}\\n{title}\\n{SECTION_DIVIDER}\\n\")\n",
        "\n",
        "def print_sub_heading(title: str):\n",
        "    print(f\"\\n{SUB_SECTION_DIVIDER}\\n{title}\\n{SUB_SECTION_DIVIDER}\\n\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 0. WORK FROM A COPY\n",
        "# ============================================================\n",
        "\n",
        "print_heading(\"Preparing Transaction Data for Feature Engineering\")\n",
        "\n",
        "tx = df.copy()\n",
        "print(\"Initial row count:\", len(tx))\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 1. BULLETPROOF TIMESTAMP PARSING\n",
        "# ============================================================\n",
        "\n",
        "# Use the raw strings\n",
        "raw_ts = tx['block_timestamp'].astype(str)\n",
        "\n",
        "# convert wei to ether (leaving commented out for now)\n",
        "# tx['value'] = tx['value'] / 1e18\n",
        "\n",
        "# Normalize known patterns\n",
        "clean_ts = (\n",
        "    raw_ts.str.replace(\" UTC\", \"\", regex=False)\n",
        "          .str.replace(\" UTC+0000\", \"\", regex=False)\n",
        "          .str.replace(\"+0000 UTC\", \"\", regex=False)\n",
        "          .str.replace(\" Z\", \"\", regex=False)\n",
        "          .str.strip()\n",
        ")\n",
        "\n",
        "# Parse everything\n",
        "parsed_ts = pd.to_datetime(clean_ts, utc=True, errors='coerce')\n",
        "\n",
        "# TRUE timestamp failures\n",
        "true_bad_mask = parsed_ts.isna()\n",
        "true_bad_count = true_bad_mask.sum()\n",
        "\n",
        "print(\"True unparseable timestamps:\", true_bad_count)\n",
        "\n",
        "if true_bad_count > 0:\n",
        "    print(\"Examples:\", raw_ts[true_bad_mask].unique()[:10])\n",
        "    median_ts = parsed_ts[~true_bad_mask].median()\n",
        "    parsed_ts[true_bad_mask] = median_ts\n",
        "\n",
        "tx['block_timestamp'] = parsed_ts\n",
        "\n",
        "# Seconds since dataset start\n",
        "global_start = tx['block_timestamp'].min()\n",
        "tx['ts_seconds'] = (tx['block_timestamp'] - global_start).dt.total_seconds()\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 2. NUMERIC CLEANING — ZERO LOSS\n",
        "# ============================================================\n",
        "\n",
        "numeric_cols = [\n",
        "    'value', 'gas', 'gas_price',\n",
        "    'receipt_cumulative_gas_used', 'receipt_gas_used'\n",
        "]\n",
        "\n",
        "print_heading(\"Numeric Column Diagnostics\")\n",
        "\n",
        "for col in numeric_cols:\n",
        "    # Check invalid values BEFORE converting\n",
        "    raw = tx[col].astype(str)\n",
        "    bad_mask = pd.to_numeric(raw, errors='coerce').isna()\n",
        "    bad_count = bad_mask.sum()\n",
        "\n",
        "    print(f\"{col}: invalid={bad_count}\")\n",
        "\n",
        "    # Convert → fill NaN with 0 → no rows dropped\n",
        "    tx[col] = pd.to_numeric(raw, errors='coerce').fillna(0)\n",
        "\n",
        "print(\"Numeric cleaning complete. Rows retained:\", len(tx))\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 3. DEGREE & AMOUNT FEATURES\n",
        "# ============================================================\n",
        "\n",
        "print_heading(\"Degree & Amount Features\")\n",
        "\n",
        "# Degrees\n",
        "in_degree = tx.groupby('to_address').size().rename('in_degree')\n",
        "out_degree = tx.groupby('from_address').size().rename('out_degree')\n",
        "all_degree = in_degree.add(out_degree, fill_value=0).rename('all_degree')\n",
        "\n",
        "unique_in_degree = (\n",
        "    tx.groupby('to_address')['from_address']\n",
        "      .nunique()\n",
        "      .rename('unique in_degree')\n",
        ")\n",
        "\n",
        "unique_out_degree = (\n",
        "    tx.groupby('from_address')['to_address']\n",
        "      .nunique()\n",
        "      .rename('unique out_degree')\n",
        ")\n",
        "\n",
        "# Amount stats\n",
        "incoming_amounts = (\n",
        "    tx.groupby('to_address')['value']\n",
        "      .agg(['mean', 'sum', 'max', 'min'])\n",
        "      .rename(columns={\n",
        "          'mean': 'Avg amount incoming',\n",
        "          'sum':  'Total amount incoming',\n",
        "          'max':  'Max amount incoming',\n",
        "          'min':  'Min amount incoming',\n",
        "      })\n",
        ")\n",
        "\n",
        "outgoing_amounts = (\n",
        "    tx.groupby('from_address')['value']\n",
        "      .agg(['mean', 'sum', 'max', 'min'])\n",
        "      .rename(columns={\n",
        "          'mean': 'Avg amount outgoing',\n",
        "          'sum':  'Total amount outgoing',\n",
        "          'max':  'Max amount outgoing',\n",
        "          'min':  'Min amount outgoing',\n",
        "      })\n",
        ")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 4. BUILD FEATURE TABLE\n",
        "# ============================================================\n",
        "\n",
        "all_addresses = (\n",
        "    pd.concat([tx['from_address'], tx['to_address']])\n",
        "      .dropna()\n",
        "      .unique()\n",
        ")\n",
        "\n",
        "features = pd.DataFrame(index=all_addresses)\n",
        "features.index.name = 'Address'\n",
        "\n",
        "# Join degree features\n",
        "features = (\n",
        "    features.join(all_degree, how='left')\n",
        "            .join(in_degree, how='left')\n",
        "            .join(out_degree, how='left')\n",
        "            .join(unique_in_degree, how='left')\n",
        "            .join(unique_out_degree, how='left')\n",
        ")\n",
        "\n",
        "# Join amount features\n",
        "features = (\n",
        "    features.join(incoming_amounts, how='left')\n",
        "            .join(outgoing_amounts, how='left')\n",
        ")\n",
        "\n",
        "# Fill 0 for missing numeric combos\n",
        "features[['all_degree', 'in_degree', 'out_degree',\n",
        "          'unique in_degree', 'unique out_degree']] = (\n",
        "    features[['all_degree', 'in_degree', 'out_degree',\n",
        "              'unique in_degree', 'unique out_degree']]\n",
        "    .fillna(0)\n",
        "    .astype(int)\n",
        ")\n",
        "\n",
        "amount_cols = [\n",
        "    'Avg amount incoming', 'Total amount incoming',\n",
        "    'Max amount incoming', 'Min amount incoming',\n",
        "    'Avg amount outgoing', 'Total amount outgoing',\n",
        "    'Max amount outgoing', 'Min amount outgoing',\n",
        "]\n",
        "\n",
        "features[amount_cols] = features[amount_cols].fillna(0)\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 5. TIME-BASED FEATURES\n",
        "# ============================================================\n",
        "\n",
        "print_heading(\"Time-Based Features\")\n",
        "\n",
        "# Long-form table for time + gas stats\n",
        "incoming_long = tx[['to_address', 'ts_seconds', 'gas', 'gas_price']].rename(\n",
        "    columns={'to_address': 'Address'}\n",
        ")\n",
        "incoming_long['direction'] = 'in'\n",
        "\n",
        "outgoing_long = tx[['from_address', 'ts_seconds', 'gas', 'gas_price']].rename(\n",
        "    columns={'from_address': 'Address'}\n",
        ")\n",
        "outgoing_long['direction'] = 'out'\n",
        "\n",
        "long_df = pd.concat([incoming_long, outgoing_long], ignore_index=True)\n",
        "long_df = long_df.dropna(subset=['Address'])\n",
        "\n",
        "\n",
        "# ---------- Avg time incoming/outgoing ----------\n",
        "avg_time_in = (\n",
        "    long_df[long_df['direction'] == 'in']\n",
        "    .groupby('Address')['ts_seconds']\n",
        "    .mean()\n",
        "    .rename('Avg time incoming')\n",
        ")\n",
        "\n",
        "avg_time_out = (\n",
        "    long_df[long_df['direction'] == 'out']\n",
        "    .groupby('Address')['ts_seconds']\n",
        "    .mean()\n",
        "    .rename('Avg time outgoing')\n",
        ")\n",
        "\n",
        "features = features.join(avg_time_in, how='left')\n",
        "features = features.join(avg_time_out, how='left')\n",
        "\n",
        "features[['Avg time incoming', 'Avg time outgoing']] = (\n",
        "    features[['Avg time incoming', 'Avg time outgoing']].fillna(0.0)\n",
        ")\n",
        "\n",
        "\n",
        "# ---------- Total transaction time, Active Duration, Intervals ----------\n",
        "def compute_time_stats(group):\n",
        "    \"\"\"Compute lifespan + time interval stats for a single address.\"\"\"\n",
        "    times = np.sort(group['ts_seconds'].values)\n",
        "    if len(times) == 0:\n",
        "        return pd.Series({\n",
        "            'Total transaction time': 0.0,\n",
        "            'Active Duration': 0.0,\n",
        "            'Mean time interval': 0.0,\n",
        "            'Max time interval': 0.0,\n",
        "            'Min time interval': 0.0,\n",
        "        })\n",
        "\n",
        "    first = times[0]\n",
        "    last = times[-1]\n",
        "    active = last - first\n",
        "\n",
        "    if len(times) > 1:\n",
        "        gaps = np.diff(times)\n",
        "        mean_gap = float(gaps.mean())\n",
        "        max_gap = float(gaps.max())\n",
        "        min_gap = float(gaps.min())\n",
        "        total_tx_time = float(gaps.sum())  # equals last - first\n",
        "    else:\n",
        "        mean_gap = max_gap = min_gap = 0.0\n",
        "        total_tx_time = 0.0\n",
        "\n",
        "    return pd.Series({\n",
        "        'Total transaction time': total_tx_time,\n",
        "        'Active Duration': active,\n",
        "        'Mean time interval': mean_gap,\n",
        "        'Max time interval': max_gap,\n",
        "        'Min time interval': min_gap,\n",
        "    })\n",
        "\n",
        "time_stats = long_df.groupby('Address').apply(compute_time_stats)\n",
        "features = features.join(time_stats, how='left')\n",
        "\n",
        "for col in [\n",
        "    'Total transaction time', 'Active Duration',\n",
        "    'Mean time interval', 'Max time interval', 'Min time interval'\n",
        "]:\n",
        "    features[col] = features[col].fillna(0.0)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 5B. ADVANCED TEMPORAL BEHAVIOR FEATURES\n",
        "# ============================================================\n",
        "\n",
        "print_heading(\"Advanced Temporal Behavior Features\")\n",
        "\n",
        "# Pre-extract times and hours\n",
        "long_df['hour'] = (global_start + pd.to_timedelta(long_df['ts_seconds'], unit='s')).dt.hour\n",
        "\n",
        "# ---- Burstiness (max_gap / median_gap) ----\n",
        "def compute_burstiness(group):\n",
        "    times = np.sort(group['ts_seconds'].values)\n",
        "    if len(times) <= 2:\n",
        "        return pd.Series({'Burstiness': 0.0})\n",
        "    gaps = np.diff(times)\n",
        "    med = np.median(gaps)\n",
        "    if med == 0:\n",
        "        return pd.Series({'Burstiness': float(gaps.max())})\n",
        "    return pd.Series({'Burstiness': float(gaps.max() / med)})\n",
        "\n",
        "burst = long_df.groupby('Address').apply(compute_burstiness)\n",
        "features = features.join(burst, how='left')\n",
        "features['Burstiness'] = features['Burstiness'].fillna(0.0)\n",
        "\n",
        "\n",
        "# ---- Activity density (#tx / active_duration) ----\n",
        "density = (\n",
        "    long_df.groupby('Address')\n",
        "           .size()\n",
        "           .rename('Tx count')\n",
        ")\n",
        "\n",
        "features = features.join(density, how='left')\n",
        "features['Tx count'] = features['Tx count'].fillna(0)\n",
        "\n",
        "features['Activity Density'] = (\n",
        "    features['Tx count'] / (features['Active Duration'] + 1)\n",
        ").astype(float)\n",
        "\n",
        "\n",
        "# ---- Incoming/outgoing ratio ----\n",
        "incoming_ct = long_df[long_df['direction']=='in'].groupby('Address').size().rename('Incoming count')\n",
        "outgoing_ct = long_df[long_df['direction']=='out'].groupby('Address').size().rename('Outgoing count')\n",
        "\n",
        "features = features.join(incoming_ct, how='left')\n",
        "features = features.join(outgoing_ct, how='left')\n",
        "\n",
        "features[['Incoming count', 'Outgoing count']] = (\n",
        "    features[['Incoming count', 'Outgoing count']].fillna(0)\n",
        ")\n",
        "\n",
        "features['In/Out Ratio'] = (\n",
        "    features['Incoming count'] / (features['Outgoing count'] + 1)\n",
        ").astype(float)\n",
        "\n",
        "\n",
        "# ---- Hour-of-day entropy + mean hour (SAFE VERSION) ----\n",
        "\n",
        "records = []\n",
        "\n",
        "for address, group in long_df.groupby('Address'):\n",
        "    hours = group['hour'].values\n",
        "    if len(hours) == 0:\n",
        "        hour_mean = 0.0\n",
        "        hour_entropy = 0.0\n",
        "    else:\n",
        "        counts = np.bincount(hours, minlength=24)\n",
        "        total = counts.sum()\n",
        "        if total == 0:\n",
        "            hour_mean = 0.0\n",
        "            hour_entropy = 0.0\n",
        "        else:\n",
        "            probs = counts / total\n",
        "            probs = probs[probs > 0]\n",
        "            hour_entropy = float(-(probs * np.log2(probs)).sum())\n",
        "            hour_mean = float(hours.mean())\n",
        "\n",
        "    records.append((address, hour_mean, hour_entropy))\n",
        "\n",
        "hour_df = pd.DataFrame(records, columns=['Address', 'Hour mean', 'Hour entropy'])\n",
        "hour_df = hour_df.set_index('Address')\n",
        "\n",
        "features = features.join(hour_df, how='left')\n",
        "\n",
        "features[['Hour mean', 'Hour entropy']] = (\n",
        "    features[['Hour mean', 'Hour entropy']].fillna(0.0)\n",
        ")\n",
        "\n",
        "\n",
        "# ---- Recency (how recently the address was active) ----\n",
        "dataset_end = tx['ts_seconds'].max()\n",
        "\n",
        "last_seen = (\n",
        "    long_df.groupby('Address')['ts_seconds']\n",
        "           .max()\n",
        "           .rename('Last seen')\n",
        ")\n",
        "\n",
        "features = features.join(last_seen, how='left')\n",
        "features['Last seen'] = features['Last seen'].fillna(0.0)\n",
        "\n",
        "features['Recency'] = dataset_end - features['Last seen']\n",
        "features['Recency'] = features['Recency'].astype(float)\n",
        "\n",
        "# ============================================================\n",
        "# 6. GAS FEATURES\n",
        "# ============================================================\n",
        "\n",
        "print_heading(\"Gas-Based Features\")\n",
        "\n",
        "avg_gas_price = (\n",
        "    long_df.groupby('Address')['gas_price']\n",
        "           .mean()\n",
        "           .rename('Avg gas price')\n",
        ")\n",
        "\n",
        "avg_gas_limit = (\n",
        "    long_df.groupby('Address')['gas']\n",
        "           .mean()\n",
        "           .rename('Avg gas limit')\n",
        ")\n",
        "\n",
        "features = features.join(avg_gas_price, how='left')\n",
        "features = features.join(avg_gas_limit, how='left')\n",
        "\n",
        "features[['Avg gas price', 'Avg gas limit']] = (\n",
        "    features[['Avg gas price', 'Avg gas limit']].fillna(0.0)\n",
        ")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 7. GRAPH CLUSTERING COEFFICIENT\n",
        "# ============================================================\n",
        "\n",
        "print_heading(\"Graph Features (Clustering Coefficient)\")\n",
        "\n",
        "# Build an undirected graph from transaction edges\n",
        "G = nx.Graph()\n",
        "edges = tx[['from_address', 'to_address']].dropna()\n",
        "G.add_edges_from(edges.itertuples(index=False, name=None))\n",
        "\n",
        "# Compute clustering coefficient per node\n",
        "cluster_series = pd.Series(nx.clustering(G), name='Clustering coefficient')\n",
        "\n",
        "features = features.join(cluster_series, how='left')\n",
        "features['Clustering coefficient'] = features['Clustering coefficient'].fillna(0.0)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 8. SCAM LABEL\n",
        "# ============================================================\n",
        "\n",
        "print_heading(\"Scam Labeling\")\n",
        "\n",
        "def is_scam_category(x):\n",
        "    if pd.isna(x): \n",
        "        return False\n",
        "    x = str(x).lower()\n",
        "    return ('scam' in x) or ('fraud' in x) or ('phish' in x)\n",
        "\n",
        "# Transaction-level flags\n",
        "tx['from_is_scam'] = (\n",
        "    (tx.get('from_scam', 0) == 1) |\n",
        "    tx['from_category'].apply(is_scam_category)\n",
        ")\n",
        "\n",
        "tx['to_is_scam'] = (\n",
        "    (tx.get('to_scam', 0) == 1) |\n",
        "    tx['to_category'].apply(is_scam_category)\n",
        ")\n",
        "\n",
        "scam_addresses = pd.Index(\n",
        "    pd.concat([\n",
        "        tx.loc[tx['from_is_scam'], 'from_address'],\n",
        "        tx.loc[tx['to_is_scam'], 'to_address']\n",
        "    ])\n",
        ").dropna().unique()\n",
        "\n",
        "scam_series = pd.Series(0, index=features.index, name='Scam')\n",
        "scam_series.loc[scam_addresses] = 1\n",
        "\n",
        "features = features.join(scam_series, how='left')\n",
        "features['Scam'] = features['Scam'].fillna(0).astype(int)\n",
        "\n",
        "# ============================================================\n",
        "# ORDER COLUMNS (Updated with advanced temporal features)\n",
        "# ============================================================\n",
        "\n",
        "ordered_cols = [\n",
        "\n",
        "    # --- degree features ---\n",
        "    'all_degree', 'in_degree', 'out_degree',\n",
        "    'unique in_degree', 'unique out_degree',\n",
        "\n",
        "    # --- amount features ---\n",
        "    'Avg amount incoming', 'Total amount incoming',\n",
        "    'Max amount incoming', 'Min amount incoming',\n",
        "    'Avg amount outgoing', 'Total amount outgoing',\n",
        "    'Max amount outgoing', 'Min amount outgoing',\n",
        "\n",
        "    # --- core time features ---\n",
        "    'Avg time incoming', 'Avg time outgoing',\n",
        "    'Total transaction time', 'Active Duration',\n",
        "    'Mean time interval', 'Max time interval', 'Min time interval',\n",
        "\n",
        "    # --- advanced temporal-behavior features ---\n",
        "    'Burstiness',\n",
        "    'Tx count', 'Activity Density',\n",
        "    'Incoming count', 'Outgoing count', 'In/Out Ratio',\n",
        "    'Hour mean', 'Hour entropy',\n",
        "    'Last seen', 'Recency',\n",
        "\n",
        "    # --- gas features ---\n",
        "    'Avg gas price', 'Avg gas limit',\n",
        "\n",
        "    # --- graph ---\n",
        "    'Clustering coefficient',\n",
        "\n",
        "    # --- target label ---\n",
        "    'Scam',\n",
        "]\n",
        "\n",
        "features = features[ordered_cols]\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# FINAL PREVIEW\n",
        "# ============================================================\n",
        "\n",
        "print_sub_heading(\"Final Feature Table Sample\")\n",
        "display(features.head())\n",
        "\n",
        "print(\"Total addresses:\", len(features))\n",
        "print(\"Total scam labels:\", features['Scam'].sum())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "a4da8c2e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "========================================\n",
            "Saving Features\n",
            "========================================\n",
            "\n",
            "Saved CSV → data/address_features.csv\n",
            "Saved Parquet → data/address_features.parquet\n"
          ]
        }
      ],
      "source": [
        "print_heading(\"Saving Features\")\n",
        "\n",
        "features_path_csv = \"data/address_features.csv\"\n",
        "features_path_parquet = \"data/address_features.parquet\"\n",
        "\n",
        "features.to_csv(features_path_csv)\n",
        "features.to_parquet(features_path_parquet)\n",
        "\n",
        "print(\"Saved CSV →\", features_path_csv)\n",
        "print(\"Saved Parquet →\", features_path_parquet)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76c93d3b",
      "metadata": {},
      "source": [
        "### Engineered Address-Level Feature Table — Feature Dictionary\n",
        "\n",
        "Index: each row corresponds to a unique Ethereum `Address` (string), aggregated over all transactions.\n",
        "\n",
        "| Field | Type | Meaning | Use | Notes |\n",
        "|---|---|---|---|---|\n",
        "| all_degree | int | Total number of tx edges (in + out) for address | analysis | in_degree + out_degree over full dataset |\n",
        "| in_degree | int | Count of incoming txs to address | analysis | Number of rows where address is `to_address` |\n",
        "| out_degree | int | Count of outgoing txs from address | analysis | Number of rows where address is `from_address` |\n",
        "| unique in_degree | int | Number of distinct senders to this address | analysis | Unique `from_address` values seen as incoming |\n",
        "| unique out_degree | int | Number of distinct recipients from this address | analysis | Unique `to_address` values seen as outgoing |\n",
        "| Avg amount incoming | float | Mean incoming transfer value (wei) | analysis | Averaged over all txs where address is recipient |\n",
        "| Total amount incoming | float | Sum of incoming transfer value (wei) | analysis | Total ETH in wei received |\n",
        "| Max amount incoming | float | Maximum single incoming value (wei) | analysis | Largest inbound transfer |\n",
        "| Min amount incoming | float | Minimum single incoming value (wei) | analysis | Smallest inbound transfer (0 if none) |\n",
        "| Avg amount outgoing | float | Mean outgoing transfer value (wei) | analysis | Averaged over all txs sent by address |\n",
        "| Total amount outgoing | float | Sum of outgoing transfer value (wei) | analysis | Total ETH in wei sent |\n",
        "| Max amount outgoing | float | Maximum single outgoing value (wei) | analysis | Largest outbound transfer |\n",
        "| Min amount outgoing | float | Minimum single outgoing value (wei) | analysis | Smallest outbound transfer (0 if none) |\n",
        "| Avg time incoming | float | Mean timestamp of incoming txs (seconds) | analysis | Seconds since earliest block in dataset |\n",
        "| Avg time outgoing | float | Mean timestamp of outgoing txs (seconds) | analysis | Seconds since earliest block in dataset |\n",
        "| Total transaction time | float | Sum of gaps between consecutive txs (s) | analysis | For this address; equals last_time − first_time when ≥2 txs |\n",
        "| Active Duration | float | Lifespan between first and last tx (s) | analysis | 0 if only a single tx |\n",
        "| Mean time interval | float | Mean gap between consecutive txs (s) | analysis | 0 if ≤1 tx |\n",
        "| Max time interval | float | Largest gap between consecutive txs (s) | analysis | 0 if ≤1 tx |\n",
        "| Min time interval | float | Smallest gap between consecutive txs (s) | analysis | 0 if ≤1 tx |\n",
        "| Burstiness | float | max_gap / median_gap of tx times | analysis | 0 for ≤2 txs; higher = more bursty activity |\n",
        "| Tx count | float | Total number of txs (in + out) | analysis | Same scale as all_degree; stored as numeric |\n",
        "| Activity Density | float | Tx count per second of Active Duration | analysis | `Tx count / (Active Duration + 1)` to avoid division by zero |\n",
        "| Incoming count | float | Number of incoming txs | analysis | Count of records where address is recipient |\n",
        "| Outgoing count | float | Number of outgoing txs | analysis | Count of records where address is sender |\n",
        "| In/Out Ratio | float | Incoming count divided by (Outgoing count + 1) | analysis | Higher values = sink-like behavior |\n",
        "| Hour mean | float | Mean hour of day of activity (0–23) | analysis | Computed from UTC timestamps across all txs |\n",
        "| Hour entropy | float | Entropy of hourly activity distribution (bits) | analysis | 0 = all txs at one hour; higher = spread across hours |\n",
        "| Last seen | float | Timestamp of most recent tx (s) | analysis | Seconds since earliest block in dataset |\n",
        "| Recency | float | How long before dataset end address was last active (s) | analysis | `dataset_end_ts_seconds − Last seen` |\n",
        "| Avg gas price | float | Mean gas price used by address (wei per gas) | analysis | Aggregated across all in/out txs |\n",
        "| Avg gas limit | float | Mean gas limit on txs involving address | analysis | Aggregated across all in/out txs |\n",
        "| Clustering coefficient | float | Local graph clustering of address | analysis | Undirected graph; range [0, 1] |\n",
        "| Scam | int (0/1) | Address labeled as scam-related | target | Derived from from_scam/to_scam and *_category text |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "4bc513fe",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "========================================\n",
            "Loading Engineered Address Features\n",
            "========================================\n",
            "\n",
            "Shape: (73034, 34)\n",
            "Columns: ['all_degree', 'in_degree', 'out_degree', 'unique in_degree', 'unique out_degree', 'Avg amount incoming', 'Total amount incoming', 'Max amount incoming', 'Min amount incoming', 'Avg amount outgoing', 'Total amount outgoing', 'Max amount outgoing', 'Min amount outgoing', 'Avg time incoming', 'Avg time outgoing', 'Total transaction time', 'Active Duration', 'Mean time interval', 'Max time interval', 'Min time interval', 'Burstiness', 'Tx count', 'Activity Density', 'Incoming count', 'Outgoing count', 'In/Out Ratio', 'Hour mean', 'Hour entropy', 'Last seen', 'Recency', 'Avg gas price', 'Avg gas limit', 'Clustering coefficient', 'Scam']\n",
            "Positive (scam) count: 169\n",
            "Negative (non-scam) count: 72865\n",
            "\n",
            "========================================\n",
            "Train / Validation / Test Split\n",
            "========================================\n",
            "\n",
            "Train size: 51123\n",
            "Val size:   10955\n",
            "Test size:  10956\n",
            "\n",
            "========================================\n",
            "Scaling Numeric Features (for Linear/NN Models)\n",
            "========================================\n",
            "\n",
            "\n",
            "========================================\n",
            "Defining Baseline Models\n",
            "========================================\n",
            "\n",
            "XGBoost not installed; skipping XGBoost baseline.\n",
            "\n",
            "========================================\n",
            "Running Baseline Models\n",
            "========================================\n",
            "\n",
            "\n",
            "----------------------------------------\n",
            "Fitting LogisticRegression\n",
            "----------------------------------------\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "75052.52s - Error patching args (debugger not attached to subprocess).\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/envs/py311/lib/python3.11/site-packages/debugpy/_vendored/pydevd/_pydev_bundle/pydev_monkey.py\", line 541, in patch_args\n",
            "    new_args.append(_get_python_c_args(host, port, code, unquoted_args, SetupHolder.setup))\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/miniconda3/envs/py311/lib/python3.11/site-packages/debugpy/_vendored/pydevd/_pydev_bundle/pydev_monkey.py\", line 193, in _get_python_c_args\n",
            "    if \"__future__\" in code:\n",
            "       ^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: a bytes-like object is required, not 'str'\n",
            "0.01s - Debugger warning: It seems that frozen modules are being used, which may\n",
            "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
            "0.00s - to python to disable frozen modules.\n",
            "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
            "0.01s - Debugger warning: It seems that frozen modules are being used, which may\n",
            "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
            "0.00s - to python to disable frozen modules.\n",
            "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
            "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
            "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
            "0.00s - to python to disable frozen modules.\n",
            "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
            "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
            "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
            "0.00s - to python to disable frozen modules.\n",
            "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
            "0.01s - Debugger warning: It seems that frozen modules are being used, which may\n",
            "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
            "0.00s - to python to disable frozen modules.\n",
            "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
            "0.01s - Debugger warning: It seems that frozen modules are being used, which may\n",
            "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
            "0.00s - to python to disable frozen modules.\n",
            "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
            "0.01s - Debugger warning: It seems that frozen modules are being used, which may\n",
            "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
            "0.00s - to python to disable frozen modules.\n",
            "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
            "0.01s - Debugger warning: It seems that frozen modules are being used, which may\n",
            "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
            "0.00s - to python to disable frozen modules.\n",
            "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
            "0.01s - Debugger warning: It seems that frozen modules are being used, which may\n",
            "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
            "0.00s - to python to disable frozen modules.\n",
            "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
            "0.46s - Expected: /opt/miniconda3/envs/py311/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd_attach_to_process/attach.dylib to exist.\n",
            "0.92s - Expected: /opt/miniconda3/envs/py311/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd_attach_to_process/attach.dylib to exist.\n",
            "0.92s - Expected: /opt/miniconda3/envs/py311/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd_attach_to_process/attach.dylib to exist.\n",
            "0.92s - Expected: /opt/miniconda3/envs/py311/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd_attach_to_process/attach.dylib to exist.\n",
            "0.92s - Expected: /opt/miniconda3/envs/py311/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd_attach_to_process/attach.dylib to exist.\n",
            "0.92s - Expected: /opt/miniconda3/envs/py311/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd_attach_to_process/attach.dylib to exist.\n",
            "0.92s - Expected: /opt/miniconda3/envs/py311/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd_attach_to_process/attach.dylib to exist.\n",
            "0.92s - Expected: /opt/miniconda3/envs/py311/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd_attach_to_process/attach.dylib to exist.\n",
            "0.92s - Expected: /opt/miniconda3/envs/py311/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd_attach_to_process/attach.dylib to exist.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scoring...\n",
            "LogisticRegression — Validation metrics:\n",
            "  val_accuracy: 0.9939\n",
            "  val_precision: 0.2667\n",
            "  val_recall: 0.9600\n",
            "  val_f1: 0.4174\n",
            "  val_roc_auc: 0.9981\n",
            "  val_avg_precision: 0.6175\n",
            "Confusion matrix (val):\n",
            "[[10864    66]\n",
            " [    1    24]]\n",
            "\n",
            "----------------------------------------\n",
            "Fitting RandomForest\n",
            "----------------------------------------\n",
            "\n",
            "Scoring...\n",
            "RandomForest — Validation metrics:\n",
            "  val_accuracy: 0.9994\n",
            "  val_precision: 0.9091\n",
            "  val_recall: 0.8000\n",
            "  val_f1: 0.8511\n",
            "  val_roc_auc: 0.9999\n",
            "  val_avg_precision: 0.9525\n",
            "Confusion matrix (val):\n",
            "[[10928     2]\n",
            " [    5    20]]\n",
            "\n",
            "----------------------------------------\n",
            "Fitting ExtraTrees\n",
            "----------------------------------------\n",
            "\n",
            "Scoring...\n",
            "ExtraTrees — Validation metrics:\n",
            "  val_accuracy: 0.9993\n",
            "  val_precision: 0.9048\n",
            "  val_recall: 0.7600\n",
            "  val_f1: 0.8261\n",
            "  val_roc_auc: 0.9998\n",
            "  val_avg_precision: 0.9286\n",
            "Confusion matrix (val):\n",
            "[[10928     2]\n",
            " [    6    19]]\n",
            "\n",
            "----------------------------------------\n",
            "Fitting MLP\n",
            "----------------------------------------\n",
            "\n",
            "Scoring...\n",
            "MLP — Validation metrics:\n",
            "  val_accuracy: 0.9994\n",
            "  val_precision: 0.9091\n",
            "  val_recall: 0.8000\n",
            "  val_f1: 0.8511\n",
            "  val_roc_auc: 0.9965\n",
            "  val_avg_precision: 0.9094\n",
            "Confusion matrix (val):\n",
            "[[10928     2]\n",
            " [    5    20]]\n",
            "\n",
            "========================================\n",
            "Baseline Model Comparison (Validation & Test)\n",
            "========================================\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>split</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1</th>\n",
              "      <th>roc_auc</th>\n",
              "      <th>avg_precision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>RandomForest</td>\n",
              "      <td>test</td>\n",
              "      <td>0.999452</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.863636</td>\n",
              "      <td>0.999618</td>\n",
              "      <td>0.922909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>ExtraTrees</td>\n",
              "      <td>test</td>\n",
              "      <td>0.999270</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>0.999583</td>\n",
              "      <td>0.906609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>LogisticRegression</td>\n",
              "      <td>test</td>\n",
              "      <td>0.993702</td>\n",
              "      <td>0.265957</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.420168</td>\n",
              "      <td>0.999107</td>\n",
              "      <td>0.716549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>MLP</td>\n",
              "      <td>test</td>\n",
              "      <td>0.998996</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.765957</td>\n",
              "      <td>0.999186</td>\n",
              "      <td>0.711810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>RandomForest</td>\n",
              "      <td>val</td>\n",
              "      <td>0.999361</td>\n",
              "      <td>0.909091</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.851064</td>\n",
              "      <td>0.999861</td>\n",
              "      <td>0.952511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ExtraTrees</td>\n",
              "      <td>val</td>\n",
              "      <td>0.999270</td>\n",
              "      <td>0.904762</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.826087</td>\n",
              "      <td>0.999806</td>\n",
              "      <td>0.928636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>MLP</td>\n",
              "      <td>val</td>\n",
              "      <td>0.999361</td>\n",
              "      <td>0.909091</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.851064</td>\n",
              "      <td>0.996494</td>\n",
              "      <td>0.909421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LogisticRegression</td>\n",
              "      <td>val</td>\n",
              "      <td>0.993884</td>\n",
              "      <td>0.266667</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0.417391</td>\n",
              "      <td>0.998112</td>\n",
              "      <td>0.617531</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                model split  accuracy  precision  recall        f1   roc_auc  \\\n",
              "3        RandomForest  test  0.999452   1.000000    0.76  0.863636  0.999618   \n",
              "5          ExtraTrees  test  0.999270   0.947368    0.72  0.818182  0.999583   \n",
              "1  LogisticRegression  test  0.993702   0.265957    1.00  0.420168  0.999107   \n",
              "7                 MLP  test  0.998996   0.818182    0.72  0.765957  0.999186   \n",
              "2        RandomForest   val  0.999361   0.909091    0.80  0.851064  0.999861   \n",
              "4          ExtraTrees   val  0.999270   0.904762    0.76  0.826087  0.999806   \n",
              "6                 MLP   val  0.999361   0.909091    0.80  0.851064  0.996494   \n",
              "0  LogisticRegression   val  0.993884   0.266667    0.96  0.417391  0.998112   \n",
              "\n",
              "   avg_precision  \n",
              "3       0.922909  \n",
              "5       0.906609  \n",
              "1       0.716549  \n",
              "7       0.711810  \n",
              "2       0.952511  \n",
              "4       0.928636  \n",
              "6       0.909421  \n",
              "0       0.617531  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# %% [markdown]\n",
        "# # Baseline Modeling — Address-Level Scam Prediction\n",
        "\n",
        "# %%\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        "    average_precision_score,\n",
        "    confusion_matrix,\n",
        ")\n",
        "\n",
        "SECTION_DIVIDER = \"=\" * 40\n",
        "SUB_SECTION_DIVIDER = \"-\" * 40\n",
        "\n",
        "def print_heading(title: str):\n",
        "    print(f\"\\n{SECTION_DIVIDER}\\n{title}\\n{SECTION_DIVIDER}\\n\")\n",
        "\n",
        "def print_sub_heading(title: str):\n",
        "    print(f\"\\n{SUB_SECTION_DIVIDER}\\n{title}\\n{SUB_SECTION_DIVIDER}\\n\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 1. LOAD ENGINEERED FEATURES\n",
        "# ============================================================\n",
        "\n",
        "print_heading(\"Loading Engineered Address Features\")\n",
        "\n",
        "features_path_csv = \"data/address_features.csv\"\n",
        "features = pd.read_csv(features_path_csv, index_col=0)\n",
        "\n",
        "print(\"Shape:\", features.shape)\n",
        "print(\"Columns:\", list(features.columns))\n",
        "\n",
        "# Target and features\n",
        "TARGET_COL = \"Scam\"\n",
        "y = features[TARGET_COL].astype(int).values\n",
        "X = features.drop(columns=[TARGET_COL])\n",
        "\n",
        "print(\"Positive (scam) count:\", int(y.sum()))\n",
        "print(\"Negative (non-scam) count:\", int((y == 0).sum()))\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 2. TRAIN / VAL / TEST SPLIT (STRATIFIED)\n",
        "# ============================================================\n",
        "\n",
        "print_heading(\"Train / Validation / Test Split\")\n",
        "\n",
        "# First: train+val vs test (15% test)\n",
        "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=0.15,\n",
        "    stratify=y,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "# Then: train vs val (val ≈ 15% of total)\n",
        "val_size_rel = 0.15 / 0.85  # so test=15%, val≈15%, train≈70%\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_trainval,\n",
        "    y_trainval,\n",
        "    test_size=val_size_rel,\n",
        "    stratify=y_trainval,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "print(\"Train size:\", X_train.shape[0])\n",
        "print(\"Val size:  \", X_val.shape[0])\n",
        "print(\"Test size: \", X_test.shape[0])\n",
        "\n",
        "# ============================================================\n",
        "# 3. SCALING FOR LINEAR / NN MODELS\n",
        "# ============================================================\n",
        "\n",
        "print_heading(\"Scaling Numeric Features (for Linear/NN Models)\")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled   = scaler.transform(X_val)\n",
        "X_test_scaled  = scaler.transform(X_test)\n",
        "\n",
        "# For trees / XGBoost we will use unscaled X_*\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 4. EVALUATION UTILITIES\n",
        "# ============================================================\n",
        "\n",
        "def get_probas(model, X):\n",
        "    \"\"\"Return probability for class 1 (scam) if possible; fall back to decision_function.\"\"\"\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        return model.predict_proba(X)[:, 1]\n",
        "    if hasattr(model, \"decision_function\"):\n",
        "        # scale to [0, 1] roughly for metrics that expect scores\n",
        "        scores = model.decision_function(X)\n",
        "        # min-max normalize\n",
        "        s_min, s_max = scores.min(), scores.max()\n",
        "        if s_max == s_min:\n",
        "            return np.zeros_like(scores)\n",
        "        return (scores - s_min) / (s_max - s_min)\n",
        "    # fallback: hard predictions -> {0,1}\n",
        "    return model.predict(X)\n",
        "\n",
        "\n",
        "def evaluate_split(y_true, y_prob, threshold=0.5):\n",
        "    \"\"\"Compute key metrics for a single split.\"\"\"\n",
        "    y_pred = (y_prob >= threshold).astype(int)\n",
        "    metrics = {}\n",
        "    metrics[\"accuracy\"] = accuracy_score(y_true, y_pred)\n",
        "    metrics[\"precision\"] = precision_score(y_true, y_pred, zero_division=0)\n",
        "    metrics[\"recall\"] = recall_score(y_true, y_pred, zero_division=0)\n",
        "    metrics[\"f1\"] = f1_score(y_true, y_pred, zero_division=0)\n",
        "\n",
        "    # Handle edge cases for AUC metrics\n",
        "    try:\n",
        "        metrics[\"roc_auc\"] = roc_auc_score(y_true, y_prob)\n",
        "    except ValueError:\n",
        "        metrics[\"roc_auc\"] = np.nan\n",
        "\n",
        "    try:\n",
        "        metrics[\"avg_precision\"] = average_precision_score(y_true, y_prob)\n",
        "    except ValueError:\n",
        "        metrics[\"avg_precision\"] = np.nan\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    metrics[\"cm\"] = cm\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def evaluate_model(name, model, X_train_in, y_train_in,\n",
        "                   X_val_in, y_val_in,\n",
        "                   X_test_in, y_test_in,\n",
        "                   threshold=0.5):\n",
        "    \"\"\"Fit model, evaluate on train/val/test, and return metrics dict.\"\"\"\n",
        "    print_sub_heading(f\"Fitting {name}\")\n",
        "    model.fit(X_train_in, y_train_in)\n",
        "\n",
        "    print(\"Scoring...\")\n",
        "    prob_train = get_probas(model, X_train_in)\n",
        "    prob_val   = get_probas(model, X_val_in)\n",
        "    prob_test  = get_probas(model, X_test_in)\n",
        "\n",
        "    train_metrics = evaluate_split(y_train_in, prob_train, threshold=threshold)\n",
        "    val_metrics   = evaluate_split(y_val_in, prob_val, threshold=threshold)\n",
        "    test_metrics  = evaluate_split(y_test_in, prob_test, threshold=threshold)\n",
        "\n",
        "    print(f\"{name} — Validation metrics:\")\n",
        "    for k in [\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\", \"avg_precision\"]:\n",
        "        print(f\"  val_{k}: {val_metrics[k]:.4f}\")\n",
        "\n",
        "    print(\"Confusion matrix (val):\")\n",
        "    print(val_metrics[\"cm\"])\n",
        "\n",
        "    return {\n",
        "        \"model_name\": name,\n",
        "        \"model\": model,\n",
        "        \"train\": train_metrics,\n",
        "        \"val\": val_metrics,\n",
        "        \"test\": test_metrics,\n",
        "    }\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 5. MODEL DEFINITIONS\n",
        "# ============================================================\n",
        "\n",
        "print_heading(\"Defining Baseline Models\")\n",
        "\n",
        "baseline_models = []\n",
        "\n",
        "# Logistic Regression (balanced)\n",
        "log_reg = LogisticRegression(\n",
        "    max_iter=1000,\n",
        "    class_weight=\"balanced\",\n",
        "    n_jobs=-1,\n",
        "    solver=\"lbfgs\",\n",
        ")\n",
        "baseline_models.append((\"LogisticRegression\", log_reg, \"scaled\"))\n",
        "\n",
        "# Random Forest\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=None,\n",
        "    n_jobs=-1,\n",
        "    class_weight=\"balanced\",\n",
        "    random_state=42,\n",
        ")\n",
        "baseline_models.append((\"RandomForest\", rf, \"raw\"))\n",
        "\n",
        "# ExtraTrees\n",
        "et = ExtraTreesClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=None,\n",
        "    n_jobs=-1,\n",
        "    class_weight=\"balanced\",\n",
        "    random_state=42,\n",
        ")\n",
        "baseline_models.append((\"ExtraTrees\", et, \"raw\"))\n",
        "\n",
        "# Try XGBoost if available\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "\n",
        "    # Class imbalance handling\n",
        "    pos = (y_train == 1).sum()\n",
        "    neg = (y_train == 0).sum()\n",
        "    scale_pos_weight = neg / max(pos, 1)\n",
        "\n",
        "    xgb = XGBClassifier(\n",
        "        n_estimators=400,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.05,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        objective=\"binary:logistic\",\n",
        "        eval_metric=\"logloss\",\n",
        "        tree_method=\"hist\",\n",
        "        n_jobs=-1,\n",
        "        scale_pos_weight=scale_pos_weight,\n",
        "        random_state=42,\n",
        "    )\n",
        "    baseline_models.append((\"XGBoost\", xgb, \"raw\"))\n",
        "    print(\"XGBoost available and added.\")\n",
        "except ImportError:\n",
        "    print(\"XGBoost not installed; skipping XGBoost baseline.\")\n",
        "\n",
        "\n",
        "# Simple MLP\n",
        "mlp = MLPClassifier(\n",
        "    hidden_layer_sizes=(64, 32),\n",
        "    activation=\"relu\",\n",
        "    solver=\"adam\",\n",
        "    alpha=1e-3,\n",
        "    batch_size=256,\n",
        "    learning_rate=\"adaptive\",\n",
        "    max_iter=50,\n",
        "    early_stopping=True,\n",
        "    random_state=42,\n",
        ")\n",
        "baseline_models.append((\"MLP\", mlp, \"scaled\"))\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 6. RUN BASELINES\n",
        "# ============================================================\n",
        "\n",
        "print_heading(\"Running Baseline Models\")\n",
        "\n",
        "results = []\n",
        "\n",
        "for name, model, space in baseline_models:\n",
        "    if space == \"scaled\":\n",
        "        Xtr, Xv, Xte = X_train_scaled, X_val_scaled, X_test_scaled\n",
        "    else:\n",
        "        Xtr, Xv, Xte = X_train, X_val, X_test\n",
        "\n",
        "    res = evaluate_model(\n",
        "        name,\n",
        "        model,\n",
        "        Xtr, y_train,\n",
        "        Xv, y_val,\n",
        "        Xte, y_test,\n",
        "        threshold=0.5,   # can tune later if needed\n",
        "    )\n",
        "    results.append(res)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 7. COMPARISON TABLE\n",
        "# ============================================================\n",
        "\n",
        "print_heading(\"Baseline Model Comparison (Validation & Test)\")\n",
        "\n",
        "rows = []\n",
        "for r in results:\n",
        "    name = r[\"model_name\"]\n",
        "    for split in [\"val\", \"test\"]:\n",
        "        m = r[split]\n",
        "        rows.append({\n",
        "            \"model\": name,\n",
        "            \"split\": split,\n",
        "            \"accuracy\": m[\"accuracy\"],\n",
        "            \"precision\": m[\"precision\"],\n",
        "            \"recall\": m[\"recall\"],\n",
        "            \"f1\": m[\"f1\"],\n",
        "            \"roc_auc\": m[\"roc_auc\"],\n",
        "            \"avg_precision\": m[\"avg_precision\"],\n",
        "        })\n",
        "\n",
        "comparison_df = pd.DataFrame(rows)\n",
        "display(comparison_df.sort_values([\"split\", \"avg_precision\"], ascending=[True, False]))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
